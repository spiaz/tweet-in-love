{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/piazza/Playground/tweet-in-love\n"
     ]
    }
   ],
   "source": [
    "%run 00.style.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from typing import List, Tuple, Iterable\n",
    "\n",
    "from tweet_in_love.settings import GlobalSettings, ModelSettings\n",
    "from tweet_in_love.preprocessor import PreProcessor\n",
    "\n",
    "settings = GlobalSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/train/train.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.train_csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(settings.train_csv_path)\n",
    "df_test = pd.read_csv(settings.test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22038</th>\n",
       "      <td>My computer died</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8868</th>\n",
       "      <td>I am sooooo happy! Finally, Kean Cipriano repl...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>gooooooodmorning world(: god bless, and have a...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>@emmarler i am jealous of your mom talking to ...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25462</th>\n",
       "      <td>@lewisking don't you procrastinate!   (like I ...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content sentiment\n",
       "22038                                   My computer died   sadness\n",
       "8868   I am sooooo happy! Finally, Kean Cipriano repl...      love\n",
       "1092   gooooooodmorning world(: god bless, and have a...     worry\n",
       "6631   @emmarler i am jealous of your mom talking to ...     worry\n",
       "25462  @lewisking don't you procrastinate!   (like I ...     worry"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download small model pretrained on web data\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK DOLLS TOMORROW NIGHT @ THE DEPOT CANC...</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@cosRobPerkins You're the third person to ask ...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@thomaskattus you asked about my SF schedule, ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SWAY SWAY BABY zommgg love it  need more screa...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@LaDyBuG21 Thanks</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  sentiment\n",
       "0  NEW YORK DOLLS TOMORROW NIGHT @ THE DEPOT CANC...        fun\n",
       "1  @cosRobPerkins You're the third person to ask ...   surprise\n",
       "2  @thomaskattus you asked about my SF schedule, ...    neutral\n",
       "3  SWAY SWAY BABY zommgg love it  need more screa...  happiness\n",
       "4                                  @LaDyBuG21 Thanks    neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just load the data in spacy's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    nlp=nlp,\n",
    "    classes=df_train.sentiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = preprocessor.df_to_doc(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = preprocessor.df_to_doc(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/train/train_web_sm.spacy'),\n",
       " PosixPath('data/test/test_web_sm.spacy'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.save_docs(train_docs, test_docs, tag=\"web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fun': 1,\n",
       " 'boredom': 0,\n",
       " 'relief': 0,\n",
       " 'anger': 0,\n",
       " 'surprise': 0,\n",
       " 'happiness': 0,\n",
       " 'neutral': 0,\n",
       " 'love': 0,\n",
       " 'sadness': 0,\n",
       " 'hate': 0,\n",
       " 'enthusiasm': 0,\n",
       " 'worry': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[0].cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Take a look at the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = preprocessor.df_to_doc(df=df_train.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my girlies @VeeVeeBOMBSHELL &amp; @ReeseCromwell r leavin me today  Vanessa, I'll c u when u'r black *I mean back and Reese, I'll c u Sunday\n",
      "(girlies @VeeVeeBOMBSHELL &, @ReeseCromwell, today, Reese, Sunday)\n",
      "['my', 'girlie', '@VeeVeeBOMBSHELL', '&', 'amp', ';', '@ReeseCromwell', 'r', 'leavin', 'I', 'today', ' ', 'Vanessa', ',', 'I', \"'ll\", 'c', 'u', 'when', \"u'r\", 'black', '*', 'I', 'mean', 'back', 'and', 'Reese', ',', 'I', \"'ll\", 'c', 'u', 'Sunday']\n",
      "[9.148404, 6.9401517, 8.537192, 9.016395, 7.8125644, 9.812474, 7.7398686, 6.605575, 6.785845, 7.818848, 9.122936, 8.097483, 7.952559, 7.6849937, 7.36979, 7.771745, 7.294192, 6.14944, 9.8892975, 6.5152082, 6.4241886, 7.3274245, 8.844927, 7.944716, 6.912108, 9.016117, 7.1334057, 7.213392, 6.9743543, 7.7682514, 7.029519, 7.044068, 9.256737]\n",
      "\n",
      "Good Morning Sunshines!\n",
      "(Good Morning Sunshines,)\n",
      "['Good', 'Morning', 'Sunshines', '!']\n",
      "[7.8765597, 7.4672213, 6.9710283, 10.281048]\n",
      "\n",
      "urgh, over slept for work, still done no revision and im SO snappy today. having a total fat day too\n",
      "(today,)\n",
      "['urgh', ',', 'over', 'sleep', 'for', 'work', ',', 'still', 'do', 'no', 'revision', 'and', 'I', 'm', 'so', 'snappy', 'today', '.', 'have', 'a', 'total', 'fat', 'day', 'too']\n",
      "[7.2118998, 7.6328993, 7.434878, 7.6518373, 7.289118, 6.492921, 8.194617, 8.740789, 6.8867445, 8.027588, 5.6421213, 7.3417296, 7.7588997, 7.1346903, 8.625368, 8.246589, 10.044519, 9.779465, 9.913813, 8.307231, 6.900699, 7.002226, 7.407217, 8.279717]\n",
      "\n",
      "@trippingtracy if there's a pow wow in the chicago area, definitely check it out, they're so fun! Happy Mother's Day\n",
      "(the chicago area,)\n",
      "['@trippingtracy', 'if', 'there', 'be', 'a', 'pow', 'wow', 'in', 'the', 'chicago', 'area', ',', 'definitely', 'check', 'it', 'out', ',', 'they', 'be', 'so', 'fun', '!', 'Happy', 'Mother', \"'s\", 'day']\n",
      "[6.264015, 7.648955, 10.1765995, 7.702578, 7.866496, 6.154043, 6.7647643, 7.1068716, 8.983694, 7.8209343, 7.492305, 8.678086, 9.750445, 7.5354705, 8.454217, 7.9619555, 7.8006024, 8.338231, 7.490884, 7.6467247, 7.3054113, 9.454943, 8.375659, 8.320744, 11.303829, 6.9203625]\n",
      "\n",
      "@cocont123 I don't know how you do it\n",
      "()\n",
      "['@cocont123', 'I', 'do', 'not', 'know', 'how', 'you', 'do', 'it']\n",
      "[7.5770683, 8.066315, 8.816485, 11.378668, 7.048822, 10.114988, 9.0290985, 7.522832, 8.267518]\n",
      "\n",
      "@paigeebaby  HAHAHAAH LAMO  thats so bad xDD i want my oneshot to go thurther hehe like ... BEDROOM xDD\n",
      "(@paigeebaby  HAHAHAAH LAMO  , xDD, BEDROOM xDD)\n",
      "['@paigeebaby', ' ', 'HAHAHAAH', 'LAMO', ' ', 'that', 's', 'so', 'bad', 'xdd', 'I', 'want', 'my', 'oneshot', 'to', 'go', 'thurther', 'hehe', 'like', '...', 'BEDROOM', 'xdd']\n",
      "[7.186792, 7.9043684, 6.914134, 7.184269, 7.190827, 8.782728, 6.4120336, 8.519778, 6.582125, 6.5054092, 7.608329, 8.095698, 8.867363, 6.874147, 8.768544, 7.16196, 7.033569, 6.8039083, 8.178711, 7.6297283, 8.279806, 7.70239]\n",
      "\n",
      "OK. Happy Star Wars Day, everyone. As they say, May the 4th be with you\n",
      "(4th,)\n",
      "['ok', '.', 'Happy', 'Star', 'Wars', 'Day', ',', 'everyone', '.', 'as', 'they', 'say', ',', 'may', 'the', '4th', 'be', 'with', 'you']\n",
      "[8.265813, 9.244469, 7.9020085, 8.29428, 7.356438, 6.694067, 7.7439947, 7.045215, 9.803082, 10.734221, 10.419425, 7.6693664, 7.612738, 8.285863, 9.372893, 6.8985076, 6.7970815, 7.2280493, 8.224314]\n",
      "\n",
      "@digital_geisha @savasavasava  For reals. It's lovely being around someone so refreshingly analog in his interests\n",
      "()\n",
      "['@digital_geisha', '@savasavasava', ' ', 'for', 'real', '.', 'it', 'be', 'lovely', 'be', 'around', 'someone', 'so', 'refreshingly', 'analog', 'in', 'his', 'interest']\n",
      "[7.4876704, 7.769739, 7.106379, 7.739995, 7.4013014, 9.647241, 8.910446, 8.177157, 7.727005, 7.920663, 6.908296, 7.063114, 8.863505, 8.739374, 6.1899996, 7.065054, 9.543963, 7.0406675]\n",
      "\n",
      "is hoping that she left her bb at home and didn't lose it on the Metro\n",
      "(Metro,)\n",
      "['be', 'hope', 'that', 'she', 'leave', 'her', 'bb', 'at', 'home', 'and', 'do', 'not', 'lose', 'it', 'on', 'the', 'Metro']\n",
      "[8.926359, 7.8554344, 8.666528, 8.763536, 7.254007, 9.300112, 6.194588, 7.252189, 7.459633, 9.419426, 9.622253, 11.524867, 7.129666, 8.272689, 7.220867, 8.327504, 7.6362343]\n",
      "\n",
      "@Thorney88 i have tried Bulmers Pear Cider - Yuk and it made me bad for a few days!!\n",
      "(a few days,)\n",
      "['@thorney88', 'I', 'have', 'try', 'Bulmers', 'Pear', 'Cider', '-', 'Yuk', 'and', 'it', 'make', 'I', 'bad', 'for', 'a', 'few', 'day', '!', '!']\n",
      "[7.9364877, 8.628941, 9.099491, 8.469774, 7.981144, 6.7608433, 7.718163, 9.782491, 6.563066, 8.150431, 8.35906, 8.486442, 9.170505, 8.371529, 7.648898, 7.394217, 8.040552, 7.5869703, 8.859619, 10.8506775]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in sample:\n",
    "    print(doc)\n",
    "    print(doc.ents)\n",
    "    print([(token.lemma_) for token in doc])\n",
    "\n",
    "    print([(token.vector_norm) for token in doc])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The vanilla pretrained model is already able to detect `@mentions`.\n",
    "  It also calculates their vector.\n",
    "- `#hashtags` are treated as words, removing the `#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x1419fad00>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add spaCyTextBlob-based sentiment analysis\n",
    "nlp2.add_pipe(\"spacytextblob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    nlp=nlp2,\n",
    "    classes=df_train.sentiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = preprocessor.df_to_doc(df=df_train.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought beer today\n",
      "(today,)\n",
      "pola:0.0 subj:0.0\n",
      "assess:[]\n",
      "..\n",
      "['buy', 'beer', 'today']\n",
      "[8.238936, 7.4964523, 10.483919]\n",
      "--\n",
      "\n",
      "@honytawk why do we need to wait another year for toy story 3?!\n",
      "(another year, 3)\n",
      "pola:0.0 subj:0.0\n",
      "assess:[]\n",
      "..\n",
      "['@honytawk', 'why', 'do', 'we', 'need', 'to', 'wait', 'another', 'year', 'for', 'toy', 'story', '3', '?', '!']\n",
      "[6.6258135, 8.821145, 9.228143, 9.22015, 7.450351, 8.512473, 7.317643, 8.592593, 6.8815374, 6.471072, 6.466468, 6.1588793, 8.160021, 9.446849, 10.280108]\n",
      "--\n",
      "\n",
      "@torriRAWR OMG apparantley green day are touring here in december..we are so going\n",
      "(green day, december)\n",
      "pola:-0.2 subj:0.3\n",
      "assess:[(['green'], -0.2, 0.3, None)]\n",
      "..\n",
      "['@torriRAWR', 'OMG', 'apparantley', 'green', 'day', 'be', 'tour', 'here', 'in', 'december', '..', 'we', 'be', 'so', 'go']\n",
      "[6.716484, 8.642971, 6.7387495, 7.5328865, 7.077684, 8.45785, 7.413875, 8.412568, 8.031971, 9.421901, 7.8196898, 7.830092, 7.292568, 8.738072, 7.3948]\n",
      "--\n",
      "\n",
      "&quot;To boldly go where no one has gone before...&quot;\n",
      "()\n",
      "pola:0.3333333333333333 subj:0.6666666666666666\n",
      "assess:[(['boldly'], 0.3333333333333333, 0.6666666666666666, None)]\n",
      "..\n",
      "['&', 'quot;To', 'boldly', 'go', 'where', 'no', 'one', 'have', 'go', 'before', '...', '&quot', ';']\n",
      "[9.541489, 8.616699, 9.106582, 6.6698575, 9.525469, 9.038959, 7.609723, 9.31208, 7.596408, 7.5737295, 8.162449, 8.5063505, 9.658854]\n",
      "--\n",
      "\n",
      "@iRobC It's a an MK mentality...I think\n",
      "(MK,)\n",
      "pola:0.0 subj:0.0\n",
      "assess:[]\n",
      "..\n",
      "['@irobc', 'it', 'be', 'a', 'an', 'MK', 'mentality', '...', 'I', 'think']\n",
      "[6.0489564, 7.8018255, 7.267795, 7.5119734, 7.0997643, 7.925912, 5.8786306, 7.827651, 8.197612, 8.237771]\n",
      "--\n",
      "\n",
      "@phomor You cant put off age but you can put off grumpiness\n",
      "()\n",
      "pola:0.0 subj:0.0\n",
      "assess:[]\n",
      "..\n",
      "['@phomor', 'you', 'can', 'not', 'put', 'off', 'age', 'but', 'you', 'can', 'put', 'off', 'grumpiness']\n",
      "[6.683282, 7.5868335, 8.941411, 8.911933, 7.5467553, 8.73963, 6.128883, 8.378348, 8.473225, 9.465313, 7.4035063, 8.909497, 6.523025]\n",
      "--\n",
      "\n",
      "@AngeDoubleYou  You can be Cinderelliiee\n",
      "(Cinderelliiee,)\n",
      "pola:0.0 subj:0.0\n",
      "assess:[]\n",
      "..\n",
      "['@angedoubleyou', ' ', 'you', 'can', 'be', 'Cinderelliiee']\n",
      "[7.1452823, 8.3876295, 7.358347, 9.724921, 8.208395, 7.224541]\n",
      "--\n",
      "\n",
      "I am loving this beautiful monday morning!\n",
      "(monday, morning)\n",
      "pola:0.8 subj:0.975\n",
      "assess:[(['loving'], 0.6, 0.95, None), (['beautiful', '!'], 1.0, 1.0, None)]\n",
      "..\n",
      "['I', 'be', 'love', 'this', 'beautiful', 'monday', 'morning', '!']\n",
      "[8.455744, 8.943918, 7.70238, 7.846064, 7.1790476, 8.464631, 8.09653, 10.446432]\n",
      "--\n",
      "\n",
      "happy momma day to myself and all my other beautiful moms out there!cant wait to get pampered tomorrow\n",
      "(tomorrow,)\n",
      "pola:0.5083333333333333 subj:0.7916666666666666\n",
      "assess:[(['happy'], 0.8, 1.0, None), (['other'], -0.125, 0.375, None), (['beautiful'], 0.85, 1.0, None)]\n",
      "..\n",
      "['happy', 'momma', 'day', 'to', 'myself', 'and', 'all', 'my', 'other', 'beautiful', 'mom', 'out', 'there!cant', 'wait', 'to', 'get', 'pamper', 'tomorrow']\n",
      "[7.549773, 6.22868, 7.2479734, 7.311029, 8.428779, 7.69254, 8.676376, 9.972573, 8.546282, 7.7956467, 6.463756, 7.9988227, 6.224177, 7.3044887, 8.33817, 7.197825, 7.9452877, 9.742955]\n",
      "--\n",
      "\n",
      "why won't the kids sleep\n",
      "()\n",
      "pola:0.0 subj:0.0\n",
      "assess:[]\n",
      "..\n",
      "['why', 'will', 'not', 'the', 'kid', 'sleep']\n",
      "[9.188222, 9.729506, 10.761256, 7.900159, 7.2009897, 6.1139827]\n",
      "--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in sample:\n",
    "    print(doc)\n",
    "    print(doc.ents)\n",
    "    print(\n",
    "        f\"pola:{doc._.polarity} subj:{doc._.subjectivity}\\nassess:{doc._.assessments}\"\n",
    "    )\n",
    "    print(\"..\")\n",
    "    print([(token.lemma_) for token in doc])\n",
    "    print([(token.vector_norm) for token in doc])\n",
    "\n",
    "    print(\"--\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanilla TextBlob is already providing some nice data points to build a baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = preprocessor.df_to_doc(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = preprocessor.df_to_doc(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/train/train_txtblob_sm.spacy'),\n",
       " PosixPath('data/test/test_txtblob_sm.spacy'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.save_docs(train_docs, test_docs, tag=\"txtblob_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify the text\n",
    "I'm gonna try to simplify the text by:\n",
    "- translate emojis\n",
    "- spell check the text\n",
    "- remove mentions\n",
    "- remove stopwords\n",
    "- remove websites\n",
    "- extract lemma\n",
    "- set lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpc = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacymoji.Emoji at 0x12a295850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacymoji import Emoji\n",
    "\n",
    "emoji = Emoji(nlpc)\n",
    "nlpc.add_pipe(\"emoji\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlpc(\"This is a test üòª üëçüèø\")\n",
    "assert doc._.has_emoji == True\n",
    "assert doc[2:5]._.has_emoji == True\n",
    "assert doc[0]._.is_emoji == False\n",
    "assert doc[4]._.is_emoji == True\n",
    "assert doc[5]._.emoji_desc == \"thumbs up dark skin tone\"\n",
    "assert len(doc._.emoji) == 2\n",
    "assert doc._.emoji[1] == (\"üëçüèø\", 5, \"thumbs up dark skin tone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell-check the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a [contextual spell checker](https://spacy.io/universe/project/contextualSpellCheck) to improve the classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextualSpellCheck.contextualSpellCheck.ContextualSpellCheck at 0x103a54310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contextualSpellCheck\n",
    "\n",
    "nlpc.add_pipe(\"contextual spellchecker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    nlp=nlpc,\n",
    "    classes=df_train.sentiment,\n",
    ")\n",
    "\n",
    "checked_docs = preprocessor.df_to_doc(df=df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW YORK DOLLS TOMORROW NIGHT @ THE DEPOT CANCELLED   REFUNDS AVAILABLE AT POINT OF PURCHASE.\n",
      "NEW YORK DOLLS TOMORROW NIGHT @ THE DEPOT CANCELLED   AND AVAILABLE AT POINT OF PURCHASE.\n",
      "\n",
      "@cosRobPerkins You're the third person to ask me out on Mon afternoon ;) I'm so sorry, but I'm helping tutor my friend for C1/C2  xx\n",
      "@cosRobPerkins You're the third person to ask me out on Mon afternoon ;) I'm so sorry, but I'm helping tutor my friend for C1/C2 .\n",
      "\n",
      "@thomaskattus you asked about my SF schedule, dahling...maybe next time\n",
      "@thomaskattus you asked about my SF schedule, and...maybe next time\n",
      "\n",
      "SWAY SWAY BABY zommgg love it  need more screamo tho boys\n",
      "SWAY SWAY BABY You love it  need more from the boys\n",
      "\n",
      "@LaDyBuG21 Thanks\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare original and spell-checked tweets\n",
    "for i in checked_docs:\n",
    "    print(i)\n",
    "    print(i._.outcome_spellCheck)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Token.remove_extension(\"is_mention\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token, Doc\n",
    "\n",
    "# add extension to identify mentions\n",
    "\n",
    "# Define getter function\n",
    "def is_mention(token):\n",
    "    return token.text.startswith(\"@\")\n",
    "\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Token.set_extension(\"is_mention\", getter=is_mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for t in nlpc(\"yo to @you\"):\n",
    "    print(t._.is_mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Token.remove_extension(\"is_web\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token, Doc\n",
    "\n",
    "# add extension to identify mentions\n",
    "\n",
    "# Define getter function\n",
    "def is_web(token):\n",
    "    return token.text.startswith(\"http\")\n",
    "\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Token.set_extension(\"is_web\", getter=is_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best False\n",
      "http://yoyo True\n"
     ]
    }
   ],
   "source": [
    "for t in nlpc(\"best http://yoyo\"):\n",
    "    print(t.text, t._.is_web)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together to simplify the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Doc.remove_extension(\"simplify\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(doc):\n",
    "    # remove mentions, stopwords,, websites, set lowercase\n",
    "    tokens = [t for t in doc if not (t._.is_mention or t.is_stop or t._.is_web)]\n",
    "    # substitute emoji or lemma\n",
    "    tokij = [t._.emoji_desc if t._.is_emoji else t.lemma_ for t in tokens]\n",
    "    # join and split again (because emoji are translated to multiple words\n",
    "    return \" \".join(tokij)\n",
    "\n",
    "\n",
    "Doc.set_extension(\"simplify\", getter=simplify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(This is a test üòª üëçüèø,\n",
       " 'test smiling cat face with heart-eyes thumbs up dark skin tone')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlpc(\"This is a test üòª üëçüèø\")\n",
    "doc, doc._.simplify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(nlp=nlpc, classes=df_train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df_train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_sklearn(df, label_encoder):\n",
    "    docs = preprocessor.df_to_doc(df)\n",
    "    X = [doc._.simplify for doc in docs]\n",
    "    y = label_encoder.transform(df[\"sentiment\"])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_for_sklearn(df_train, label_encoder)\n",
    "X_test, y_test = prepare_for_sklearn(df_test, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model collection\n",
    "\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "pipe_nb = Pipeline([(\"vectorizer\", count_vector), (\"classifier\", nb)])\n",
    "\n",
    "pipe_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append((pipe_nb, \"Naive Bayes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogReg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "pipe_log = Pipeline([(\"vectorizer\", count_vector), (\"classifier\", logreg)])\n",
    "\n",
    "# model generation\n",
    "pipe_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append((pipe_log, \"Logistic Regression\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "def show_score(y_true, y_pred, name):\n",
    "    labels = label_encoder.classes_\n",
    "\n",
    "    # f1 = f1_score(y_true, y_pred, labels=labels, average='weighted')\n",
    "\n",
    "    # Model Accuracy\n",
    "    display(Markdown(f\"##{name} score\"))\n",
    "    print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "\n",
    "for model, name in models:\n",
    "    show_score(y_test, model.predict(X_test), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What influence had the preprocessing on the results?\n",
    "- TF/IDF\n",
    "- Semantic extraction of tokens\n",
    "- Try more effective models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da01abadec455ad4ff2e8fad09fcb6f1aa80f0c03ee641ba501c7d7694d44b8e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
